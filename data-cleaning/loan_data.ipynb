{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8f85d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import getpass\n",
    "username = getpass.getuser()\n",
    "\n",
    "spark= SparkSession. \\\n",
    "builder. \\\n",
    "config('spark.ui.port','0'). \\\n",
    "config(\"spark.sql.warehouse.dir\", f\"/user/{username}/warehouse\"). \\\n",
    "enableHiveSupport(). \\\n",
    "master('yarn'). \\\n",
    "getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "735968e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_schema = \"\"\"loan_id string, member_id string, loan_amount float, \n",
    "funded_amount float, loan_term_months string, interest_rate float, \n",
    "monthly_installment float, issue_date string, loan_status string, \n",
    "loan_purpose string, loan_title string\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "338ab88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_raw_df = spark.read \\\n",
    ".format(\"csv\") \\\n",
    ".option(\"header\",True) \\\n",
    ".schema(loan_schema) \\\n",
    ".load(\"/user/itv015703/lendingclubproject/raw/loans_data_csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b7a7a3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_id</th><th>member_id</th><th>loan_amount</th><th>funded_amount</th><th>loan_term_months</th><th>interest_rate</th><th>monthly_installment</th><th>issue_date</th><th>loan_status</th><th>loan_purpose</th><th>loan_title</th></tr>\n",
       "<tr><td>94761152</td><td>1ae08eb0c074de6fb...</td><td>26975.0</td><td>26975.0</td><td>60 months</td><td>30.84</td><td>886.71</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>94093520</td><td>82ae44edab97e1c95...</td><td>28000.0</td><td>28000.0</td><td>60 months</td><td>7.99</td><td>567.61</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td></tr>\n",
       "<tr><td>95126205</td><td>7a92464a7eef5fa98...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>12.74</td><td>402.83</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95217505</td><td>6fc2a824c46b88831...</td><td>9600.0</td><td>9600.0</td><td>36 months</td><td>13.99</td><td>328.06</td><td>Dec-2016</td><td>Late (31-120 days)</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95096173</td><td>d72283a720a1bb13c...</td><td>2700.0</td><td>2700.0</td><td>36 months</td><td>8.24</td><td>84.91</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td></tr>\n",
       "<tr><td>95148408</td><td>0b0616b87c7476b53...</td><td>22400.0</td><td>22400.0</td><td>36 months</td><td>7.24</td><td>694.11</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>93793227</td><td>b89d45166d32ba422...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>15.99</td><td>421.83</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td></tr>\n",
       "<tr><td>94566736</td><td>4f170572ef915ba17...</td><td>35000.0</td><td>35000.0</td><td>60 months</td><td>13.99</td><td>814.21</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95256545</td><td>30ca62c00e346e480...</td><td>5600.0</td><td>5600.0</td><td>36 months</td><td>13.99</td><td>191.37</td><td>Dec-2016</td><td>Current</td><td>other</td><td>Other</td></tr>\n",
       "<tr><td>95272111</td><td>64f06e51bc16579dd...</td><td>5000.0</td><td>5000.0</td><td>36 months</td><td>7.99</td><td>156.66</td><td>Dec-2016</td><td>Fully Paid</td><td>medical</td><td>Medical expenses</td></tr>\n",
       "<tr><td>94991209</td><td>ad7fbe48087127d3c...</td><td>4800.0</td><td>4800.0</td><td>36 months</td><td>11.49</td><td>158.27</td><td>Dec-2016</td><td>Charged Off</td><td>moving</td><td>Moving and reloca...</td></tr>\n",
       "<tr><td>95138317</td><td>c63ea8bfa74eb9fee...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95258184</td><td>6261c9ac4c73d793a...</td><td>35000.0</td><td>35000.0</td><td>36 months</td><td>11.49</td><td>1154.0</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>94442821</td><td>77ddfeb1dc3604ae8...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td></tr>\n",
       "<tr><td>95026238</td><td>87817073693780dc9...</td><td>2000.0</td><td>2000.0</td><td>36 months</td><td>11.49</td><td>65.95</td><td>Dec-2016</td><td>Fully Paid</td><td>major_purchase</td><td>Major purchase</td></tr>\n",
       "<tr><td>94071755</td><td>f9fbcdf22f2b725da...</td><td>4000.0</td><td>4000.0</td><td>36 months</td><td>11.44</td><td>131.79</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>94626730</td><td>63c83fce390eeff35...</td><td>11650.0</td><td>11650.0</td><td>36 months</td><td>21.49</td><td>441.86</td><td>Dec-2016</td><td>Charged Off</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95196463</td><td>666aa84393abf271d...</td><td>16000.0</td><td>16000.0</td><td>60 months</td><td>12.74</td><td>361.93</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td></tr>\n",
       "<tr><td>95056184</td><td>d996c079558e0e2e6...</td><td>9000.0</td><td>9000.0</td><td>36 months</td><td>11.44</td><td>296.53</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "<tr><td>95186653</td><td>a1a6c2886fb3048c4...</td><td>6500.0</td><td>6500.0</td><td>36 months</td><td>8.24</td><td>204.41</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+\n",
       "| loan_id|           member_id|loan_amount|funded_amount|loan_term_months|interest_rate|monthly_installment|issue_date|       loan_status|      loan_purpose|          loan_title|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+\n",
       "|94761152|1ae08eb0c074de6fb...|    26975.0|      26975.0|       60 months|        30.84|             886.71|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|\n",
       "|94093520|82ae44edab97e1c95...|    28000.0|      28000.0|       60 months|         7.99|             567.61|  Dec-2016|           Current|       credit_card|Credit card refin...|\n",
       "|95126205|7a92464a7eef5fa98...|    12000.0|      12000.0|       36 months|        12.74|             402.83|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "|95217505|6fc2a824c46b88831...|     9600.0|       9600.0|       36 months|        13.99|             328.06|  Dec-2016|Late (31-120 days)|debt_consolidation|  Debt consolidation|\n",
       "|95096173|d72283a720a1bb13c...|     2700.0|       2700.0|       36 months|         8.24|              84.91|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|\n",
       "|95148408|0b0616b87c7476b53...|    22400.0|      22400.0|       36 months|         7.24|             694.11|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|\n",
       "|93793227|b89d45166d32ba422...|    12000.0|      12000.0|       36 months|        15.99|             421.83|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|\n",
       "|94566736|4f170572ef915ba17...|    35000.0|      35000.0|       60 months|        13.99|             814.21|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "|95256545|30ca62c00e346e480...|     5600.0|       5600.0|       36 months|        13.99|             191.37|  Dec-2016|           Current|             other|               Other|\n",
       "|95272111|64f06e51bc16579dd...|     5000.0|       5000.0|       36 months|         7.99|             156.66|  Dec-2016|        Fully Paid|           medical|    Medical expenses|\n",
       "|94991209|ad7fbe48087127d3c...|     4800.0|       4800.0|       36 months|        11.49|             158.27|  Dec-2016|       Charged Off|            moving|Moving and reloca...|\n",
       "|95138317|c63ea8bfa74eb9fee...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "|95258184|6261c9ac4c73d793a...|    35000.0|      35000.0|       36 months|        11.49|             1154.0|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "|94442821|77ddfeb1dc3604ae8...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|       credit_card|Credit card refin...|\n",
       "|95026238|87817073693780dc9...|     2000.0|       2000.0|       36 months|        11.49|              65.95|  Dec-2016|        Fully Paid|    major_purchase|      Major purchase|\n",
       "|94071755|f9fbcdf22f2b725da...|     4000.0|       4000.0|       36 months|        11.44|             131.79|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "|94626730|63c83fce390eeff35...|    11650.0|      11650.0|       36 months|        21.49|             441.86|  Dec-2016|       Charged Off|debt_consolidation|  Debt consolidation|\n",
       "|95196463|666aa84393abf271d...|    16000.0|      16000.0|       60 months|        12.74|             361.93|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|\n",
       "|95056184|d996c079558e0e2e6...|     9000.0|       9000.0|       36 months|        11.44|             296.53|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|\n",
       "|95186653|a1a6c2886fb3048c4...|     6500.0|       6500.0|       36 months|         8.24|             204.41|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_raw_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbec37b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amount: float (nullable = true)\n",
      " |-- funded_amount: float (nullable = true)\n",
      " |-- loan_term_months: string (nullable = true)\n",
      " |-- interest_rate: float (nullable = true)\n",
      " |-- monthly_installment: float (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_purpose: string (nullable = true)\n",
      " |-- loan_title: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_raw_df .printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e003850",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "loans_ingested_df = loans_raw_df.withColumn(\"ingest_date\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6bfd4d4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_id</th><th>member_id</th><th>loan_amount</th><th>funded_amount</th><th>loan_term_months</th><th>interest_rate</th><th>monthly_installment</th><th>issue_date</th><th>loan_status</th><th>loan_purpose</th><th>loan_title</th><th>ingest_date</th></tr>\n",
       "<tr><td>94761152</td><td>1ae08eb0c074de6fb...</td><td>26975.0</td><td>26975.0</td><td>60 months</td><td>30.84</td><td>886.71</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94093520</td><td>82ae44edab97e1c95...</td><td>28000.0</td><td>28000.0</td><td>60 months</td><td>7.99</td><td>567.61</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95126205</td><td>7a92464a7eef5fa98...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>12.74</td><td>402.83</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95217505</td><td>6fc2a824c46b88831...</td><td>9600.0</td><td>9600.0</td><td>36 months</td><td>13.99</td><td>328.06</td><td>Dec-2016</td><td>Late (31-120 days)</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95096173</td><td>d72283a720a1bb13c...</td><td>2700.0</td><td>2700.0</td><td>36 months</td><td>8.24</td><td>84.91</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95148408</td><td>0b0616b87c7476b53...</td><td>22400.0</td><td>22400.0</td><td>36 months</td><td>7.24</td><td>694.11</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>93793227</td><td>b89d45166d32ba422...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>15.99</td><td>421.83</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94566736</td><td>4f170572ef915ba17...</td><td>35000.0</td><td>35000.0</td><td>60 months</td><td>13.99</td><td>814.21</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95256545</td><td>30ca62c00e346e480...</td><td>5600.0</td><td>5600.0</td><td>36 months</td><td>13.99</td><td>191.37</td><td>Dec-2016</td><td>Current</td><td>other</td><td>Other</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95272111</td><td>64f06e51bc16579dd...</td><td>5000.0</td><td>5000.0</td><td>36 months</td><td>7.99</td><td>156.66</td><td>Dec-2016</td><td>Fully Paid</td><td>medical</td><td>Medical expenses</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94991209</td><td>ad7fbe48087127d3c...</td><td>4800.0</td><td>4800.0</td><td>36 months</td><td>11.49</td><td>158.27</td><td>Dec-2016</td><td>Charged Off</td><td>moving</td><td>Moving and reloca...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95138317</td><td>c63ea8bfa74eb9fee...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95258184</td><td>6261c9ac4c73d793a...</td><td>35000.0</td><td>35000.0</td><td>36 months</td><td>11.49</td><td>1154.0</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94442821</td><td>77ddfeb1dc3604ae8...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95026238</td><td>87817073693780dc9...</td><td>2000.0</td><td>2000.0</td><td>36 months</td><td>11.49</td><td>65.95</td><td>Dec-2016</td><td>Fully Paid</td><td>major_purchase</td><td>Major purchase</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94071755</td><td>f9fbcdf22f2b725da...</td><td>4000.0</td><td>4000.0</td><td>36 months</td><td>11.44</td><td>131.79</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94626730</td><td>63c83fce390eeff35...</td><td>11650.0</td><td>11650.0</td><td>36 months</td><td>21.49</td><td>441.86</td><td>Dec-2016</td><td>Charged Off</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95196463</td><td>666aa84393abf271d...</td><td>16000.0</td><td>16000.0</td><td>60 months</td><td>12.74</td><td>361.93</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95056184</td><td>d996c079558e0e2e6...</td><td>9000.0</td><td>9000.0</td><td>36 months</td><td>11.44</td><td>296.53</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95186653</td><td>a1a6c2886fb3048c4...</td><td>6500.0</td><td>6500.0</td><td>36 months</td><td>8.24</td><td>204.41</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "| loan_id|           member_id|loan_amount|funded_amount|loan_term_months|interest_rate|monthly_installment|issue_date|       loan_status|      loan_purpose|          loan_title|         ingest_date|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "|94761152|1ae08eb0c074de6fb...|    26975.0|      26975.0|       60 months|        30.84|             886.71|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94093520|82ae44edab97e1c95...|    28000.0|      28000.0|       60 months|         7.99|             567.61|  Dec-2016|           Current|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95126205|7a92464a7eef5fa98...|    12000.0|      12000.0|       36 months|        12.74|             402.83|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95217505|6fc2a824c46b88831...|     9600.0|       9600.0|       36 months|        13.99|             328.06|  Dec-2016|Late (31-120 days)|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95096173|d72283a720a1bb13c...|     2700.0|       2700.0|       36 months|         8.24|              84.91|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95148408|0b0616b87c7476b53...|    22400.0|      22400.0|       36 months|         7.24|             694.11|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|93793227|b89d45166d32ba422...|    12000.0|      12000.0|       36 months|        15.99|             421.83|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|94566736|4f170572ef915ba17...|    35000.0|      35000.0|       60 months|        13.99|             814.21|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95256545|30ca62c00e346e480...|     5600.0|       5600.0|       36 months|        13.99|             191.37|  Dec-2016|           Current|             other|               Other|2025-03-30 06:49:...|\n",
       "|95272111|64f06e51bc16579dd...|     5000.0|       5000.0|       36 months|         7.99|             156.66|  Dec-2016|        Fully Paid|           medical|    Medical expenses|2025-03-30 06:49:...|\n",
       "|94991209|ad7fbe48087127d3c...|     4800.0|       4800.0|       36 months|        11.49|             158.27|  Dec-2016|       Charged Off|            moving|Moving and reloca...|2025-03-30 06:49:...|\n",
       "|95138317|c63ea8bfa74eb9fee...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95258184|6261c9ac4c73d793a...|    35000.0|      35000.0|       36 months|        11.49|             1154.0|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94442821|77ddfeb1dc3604ae8...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95026238|87817073693780dc9...|     2000.0|       2000.0|       36 months|        11.49|              65.95|  Dec-2016|        Fully Paid|    major_purchase|      Major purchase|2025-03-30 06:49:...|\n",
       "|94071755|f9fbcdf22f2b725da...|     4000.0|       4000.0|       36 months|        11.44|             131.79|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94626730|63c83fce390eeff35...|    11650.0|      11650.0|       36 months|        21.49|             441.86|  Dec-2016|       Charged Off|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95196463|666aa84393abf271d...|    16000.0|      16000.0|       60 months|        12.74|             361.93|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95056184|d996c079558e0e2e6...|     9000.0|       9000.0|       36 months|        11.44|             296.53|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95186653|a1a6c2886fb3048c4...|     6500.0|       6500.0|       36 months|         8.24|             204.41|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_ingested_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8acfd8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_ingested_df.createOrReplaceTempView(\"loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e8d8914a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_id</th><th>member_id</th><th>loan_amount</th><th>funded_amount</th><th>loan_term_months</th><th>interest_rate</th><th>monthly_installment</th><th>issue_date</th><th>loan_status</th><th>loan_purpose</th><th>loan_title</th><th>ingest_date</th></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Loans that do not...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>Total amount fund...</td><td>e3b0c44298fc1c149...</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>null</td><td>2025-03-30 06:49:...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+-----------+------------+----------+--------------------+\n",
       "|             loan_id|           member_id|loan_amount|funded_amount|loan_term_months|interest_rate|monthly_installment|issue_date|loan_status|loan_purpose|loan_title|         ingest_date|\n",
       "+--------------------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+-----------+------------+----------+--------------------+\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Loans that do not...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "|Total amount fund...|e3b0c44298fc1c149...|       null|         null|            null|         null|               null|      null|       null|        null|      null|2025-03-30 06:49:...|\n",
       "+--------------------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+-----------+------------+----------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select * from loans where loan_amount is null\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aed65b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_check = [\"loan_amount\", \"funded_amount\", \"loan_term_months\", \"interest_rate\", \"monthly_installment\", \"issue_date\", \"loan_status\", \"loan_purpose\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e4f2d12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_filtered_df = loans_ingested_df.na.drop(subset=columns_to_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "834c7909",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_id</th><th>member_id</th><th>loan_amount</th><th>funded_amount</th><th>loan_term_months</th><th>interest_rate</th><th>monthly_installment</th><th>issue_date</th><th>loan_status</th><th>loan_purpose</th><th>loan_title</th><th>ingest_date</th></tr>\n",
       "<tr><td>94761152</td><td>1ae08eb0c074de6fb...</td><td>26975.0</td><td>26975.0</td><td>60 months</td><td>30.84</td><td>886.71</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94093520</td><td>82ae44edab97e1c95...</td><td>28000.0</td><td>28000.0</td><td>60 months</td><td>7.99</td><td>567.61</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95126205</td><td>7a92464a7eef5fa98...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>12.74</td><td>402.83</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95217505</td><td>6fc2a824c46b88831...</td><td>9600.0</td><td>9600.0</td><td>36 months</td><td>13.99</td><td>328.06</td><td>Dec-2016</td><td>Late (31-120 days)</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95096173</td><td>d72283a720a1bb13c...</td><td>2700.0</td><td>2700.0</td><td>36 months</td><td>8.24</td><td>84.91</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95148408</td><td>0b0616b87c7476b53...</td><td>22400.0</td><td>22400.0</td><td>36 months</td><td>7.24</td><td>694.11</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>93793227</td><td>b89d45166d32ba422...</td><td>12000.0</td><td>12000.0</td><td>36 months</td><td>15.99</td><td>421.83</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94566736</td><td>4f170572ef915ba17...</td><td>35000.0</td><td>35000.0</td><td>60 months</td><td>13.99</td><td>814.21</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95256545</td><td>30ca62c00e346e480...</td><td>5600.0</td><td>5600.0</td><td>36 months</td><td>13.99</td><td>191.37</td><td>Dec-2016</td><td>Current</td><td>other</td><td>Other</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95272111</td><td>64f06e51bc16579dd...</td><td>5000.0</td><td>5000.0</td><td>36 months</td><td>7.99</td><td>156.66</td><td>Dec-2016</td><td>Fully Paid</td><td>medical</td><td>Medical expenses</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94991209</td><td>ad7fbe48087127d3c...</td><td>4800.0</td><td>4800.0</td><td>36 months</td><td>11.49</td><td>158.27</td><td>Dec-2016</td><td>Charged Off</td><td>moving</td><td>Moving and reloca...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95138317</td><td>c63ea8bfa74eb9fee...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95258184</td><td>6261c9ac4c73d793a...</td><td>35000.0</td><td>35000.0</td><td>36 months</td><td>11.49</td><td>1154.0</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94442821</td><td>77ddfeb1dc3604ae8...</td><td>8000.0</td><td>8000.0</td><td>36 months</td><td>11.49</td><td>263.78</td><td>Dec-2016</td><td>Current</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95026238</td><td>87817073693780dc9...</td><td>2000.0</td><td>2000.0</td><td>36 months</td><td>11.49</td><td>65.95</td><td>Dec-2016</td><td>Fully Paid</td><td>major_purchase</td><td>Major purchase</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94071755</td><td>f9fbcdf22f2b725da...</td><td>4000.0</td><td>4000.0</td><td>36 months</td><td>11.44</td><td>131.79</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>94626730</td><td>63c83fce390eeff35...</td><td>11650.0</td><td>11650.0</td><td>36 months</td><td>21.49</td><td>441.86</td><td>Dec-2016</td><td>Charged Off</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95196463</td><td>666aa84393abf271d...</td><td>16000.0</td><td>16000.0</td><td>60 months</td><td>12.74</td><td>361.93</td><td>Dec-2016</td><td>Fully Paid</td><td>credit_card</td><td>Credit card refin...</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95056184</td><td>d996c079558e0e2e6...</td><td>9000.0</td><td>9000.0</td><td>36 months</td><td>11.44</td><td>296.53</td><td>Dec-2016</td><td>Fully Paid</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "<tr><td>95186653</td><td>a1a6c2886fb3048c4...</td><td>6500.0</td><td>6500.0</td><td>36 months</td><td>8.24</td><td>204.41</td><td>Dec-2016</td><td>Current</td><td>debt_consolidation</td><td>Debt consolidation</td><td>2025-03-30 06:49:...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "| loan_id|           member_id|loan_amount|funded_amount|loan_term_months|interest_rate|monthly_installment|issue_date|       loan_status|      loan_purpose|          loan_title|         ingest_date|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "|94761152|1ae08eb0c074de6fb...|    26975.0|      26975.0|       60 months|        30.84|             886.71|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94093520|82ae44edab97e1c95...|    28000.0|      28000.0|       60 months|         7.99|             567.61|  Dec-2016|           Current|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95126205|7a92464a7eef5fa98...|    12000.0|      12000.0|       36 months|        12.74|             402.83|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95217505|6fc2a824c46b88831...|     9600.0|       9600.0|       36 months|        13.99|             328.06|  Dec-2016|Late (31-120 days)|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95096173|d72283a720a1bb13c...|     2700.0|       2700.0|       36 months|         8.24|              84.91|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95148408|0b0616b87c7476b53...|    22400.0|      22400.0|       36 months|         7.24|             694.11|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|93793227|b89d45166d32ba422...|    12000.0|      12000.0|       36 months|        15.99|             421.83|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|94566736|4f170572ef915ba17...|    35000.0|      35000.0|       60 months|        13.99|             814.21|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95256545|30ca62c00e346e480...|     5600.0|       5600.0|       36 months|        13.99|             191.37|  Dec-2016|           Current|             other|               Other|2025-03-30 06:49:...|\n",
       "|95272111|64f06e51bc16579dd...|     5000.0|       5000.0|       36 months|         7.99|             156.66|  Dec-2016|        Fully Paid|           medical|    Medical expenses|2025-03-30 06:49:...|\n",
       "|94991209|ad7fbe48087127d3c...|     4800.0|       4800.0|       36 months|        11.49|             158.27|  Dec-2016|       Charged Off|            moving|Moving and reloca...|2025-03-30 06:49:...|\n",
       "|95138317|c63ea8bfa74eb9fee...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95258184|6261c9ac4c73d793a...|    35000.0|      35000.0|       36 months|        11.49|             1154.0|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94442821|77ddfeb1dc3604ae8...|     8000.0|       8000.0|       36 months|        11.49|             263.78|  Dec-2016|           Current|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95026238|87817073693780dc9...|     2000.0|       2000.0|       36 months|        11.49|              65.95|  Dec-2016|        Fully Paid|    major_purchase|      Major purchase|2025-03-30 06:49:...|\n",
       "|94071755|f9fbcdf22f2b725da...|     4000.0|       4000.0|       36 months|        11.44|             131.79|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|94626730|63c83fce390eeff35...|    11650.0|      11650.0|       36 months|        21.49|             441.86|  Dec-2016|       Charged Off|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95196463|666aa84393abf271d...|    16000.0|      16000.0|       60 months|        12.74|             361.93|  Dec-2016|        Fully Paid|       credit_card|Credit card refin...|2025-03-30 06:49:...|\n",
       "|95056184|d996c079558e0e2e6...|     9000.0|       9000.0|       36 months|        11.44|             296.53|  Dec-2016|        Fully Paid|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "|95186653|a1a6c2886fb3048c4...|     6500.0|       6500.0|       36 months|         8.24|             204.41|  Dec-2016|           Current|debt_consolidation|  Debt consolidation|2025-03-30 06:49:...|\n",
       "+--------+--------------------+-----------+-------------+----------------+-------------+-------------------+----------+------------------+------------------+--------------------+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loans_filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b668833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_filtered_df.createOrReplaceTempView(\"loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "007668c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, col\n",
    "loans_terms_modified= loans_filtered_df.withColumn(\"loan_term_months\", (regexp_replace(col(\"loan_term_months\"), \" months\", \"\") \\\n",
    "                                         .cast(\"int\")/12) \\\n",
    "                                         .cast(\"int\")) \\\n",
    "                                         .withColumnRenamed(\"loan_term_months\", \"loan_term_years\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aeee159c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- loan_id: string (nullable = true)\n",
      " |-- member_id: string (nullable = true)\n",
      " |-- loan_amount: float (nullable = true)\n",
      " |-- funded_amount: float (nullable = true)\n",
      " |-- loan_term_years: integer (nullable = true)\n",
      " |-- interest_rate: float (nullable = true)\n",
      " |-- monthly_installment: float (nullable = true)\n",
      " |-- issue_date: string (nullable = true)\n",
      " |-- loan_status: string (nullable = true)\n",
      " |-- loan_purpose: string (nullable = true)\n",
      " |-- loan_title: string (nullable = true)\n",
      " |-- ingest_date: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loans_terms_modified.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a4ae5cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "loans_terms_modified.createOrReplaceTempView(\"loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03328004",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_purpose</th></tr>\n",
       "<tr><td>guaranteed!&quot;</td></tr>\n",
       "<tr><td>and if they are a...</td></tr>\n",
       "<tr><td>never had any tro...</td></tr>\n",
       "<tr><td>&lt;br/&gt;&lt;br/&gt;Lending...</td></tr>\n",
       "<tr><td>Bank of America c...</td></tr>\n",
       "<tr><td>stocks</td></tr>\n",
       "<tr><td>please feel free ...</td></tr>\n",
       "<tr><td>I became his prim...</td></tr>\n",
       "<tr><td>brakes</td></tr>\n",
       "<tr><td>on one of the bus...</td></tr>\n",
       "<tr><td>because of the na...</td></tr>\n",
       "<tr><td>progressive multi...</td></tr>\n",
       "<tr><td>but not much info...</td></tr>\n",
       "<tr><td>I have eliminated...</td></tr>\n",
       "<tr><td>TutoringOne is a ...</td></tr>\n",
       "<tr><td>so I have to pay ...</td></tr>\n",
       "<tr><td>you may feel conf...</td></tr>\n",
       "<tr><td>Hilal Khalil Homa...</td></tr>\n",
       "<tr><td>000 in debt on cr...</td></tr>\n",
       "<tr><td>and the monthly p...</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+\n",
       "|        loan_purpose|\n",
       "+--------------------+\n",
       "|        guaranteed!\"|\n",
       "|and if they are a...|\n",
       "|never had any tro...|\n",
       "|<br/><br/>Lending...|\n",
       "|Bank of America c...|\n",
       "|              stocks|\n",
       "|please feel free ...|\n",
       "|I became his prim...|\n",
       "|              brakes|\n",
       "|on one of the bus...|\n",
       "|because of the na...|\n",
       "|progressive multi...|\n",
       "|but not much info...|\n",
       "|I have eliminated...|\n",
       "|TutoringOne is a ...|\n",
       "|you may feel conf...|\n",
       "|so I have to pay ...|\n",
       "|Hilal Khalil Homa...|\n",
       "|000 in debt on cr...|\n",
       "|and the monthly p...|\n",
       "+--------------------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select distinct(loan_purpose) from loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "930241db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>loan_purpose</th><th>total</th></tr>\n",
       "<tr><td>debt_consolidation</td><td>1277790</td></tr>\n",
       "<tr><td>credit_card</td><td>516926</td></tr>\n",
       "<tr><td>home_improvement</td><td>150440</td></tr>\n",
       "<tr><td>other</td><td>139413</td></tr>\n",
       "<tr><td>major_purchase</td><td>50429</td></tr>\n",
       "<tr><td>medical</td><td>27481</td></tr>\n",
       "<tr><td>small_business</td><td>24659</td></tr>\n",
       "<tr><td>car</td><td>24009</td></tr>\n",
       "<tr><td>vacation</td><td>15525</td></tr>\n",
       "<tr><td>moving</td><td>15402</td></tr>\n",
       "<tr><td>house</td><td>14131</td></tr>\n",
       "<tr><td>wedding</td><td>2351</td></tr>\n",
       "<tr><td>renewable_energy</td><td>1445</td></tr>\n",
       "<tr><td>educational</td><td>412</td></tr>\n",
       "<tr><td>progressive multi...</td><td>1</td></tr>\n",
       "<tr><td>and if they are a...</td><td>1</td></tr>\n",
       "<tr><td>please feel free ...</td><td>1</td></tr>\n",
       "<tr><td>never had any tro...</td><td>1</td></tr>\n",
       "<tr><td>I became his prim...</td><td>1</td></tr>\n",
       "<tr><td>brakes</td><td>1</td></tr>\n",
       "</table>\n",
       "only showing top 20 rows\n"
      ],
      "text/plain": [
       "+--------------------+-------+\n",
       "|        loan_purpose|  total|\n",
       "+--------------------+-------+\n",
       "|  debt_consolidation|1277790|\n",
       "|         credit_card| 516926|\n",
       "|    home_improvement| 150440|\n",
       "|               other| 139413|\n",
       "|      major_purchase|  50429|\n",
       "|             medical|  27481|\n",
       "|      small_business|  24659|\n",
       "|                 car|  24009|\n",
       "|            vacation|  15525|\n",
       "|              moving|  15402|\n",
       "|               house|  14131|\n",
       "|             wedding|   2351|\n",
       "|    renewable_energy|   1445|\n",
       "|         educational|    412|\n",
       "|<br/><br/>Lending...|      1|\n",
       "|please feel free ...|      1|\n",
       "|but not much info...|      1|\n",
       "|I became his prim...|      1|\n",
       "|I have eliminated...|      1|\n",
       "|progressive multi...|      1|\n",
       "+--------------------+-------+\n",
       "only showing top 20 rows"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select loan_purpose, count(*) as total from loans group by loan_purpose order by total desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7e875665",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_purpose_lookup = [\"debt_consolidation\", \"credit_card\", \"home_improvement\", \"other\", \"major_purchase\", \"medical\", \"small_business\", \"car\", \"vacation\", \"moving\", \"house\", \"wedding\", \"renewable_energy\", \"educational\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "286ad29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when, col\n",
    "\n",
    "loan_purpose_modified_df = loans_terms_modified.withColumn(\"loan_purpose\", \\\n",
    "                                when(col(\"loan_purpose\").isin(loan_purpose_lookup), col(\"loan_purpose\")).\\\n",
    "                                otherwise(\"other\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6208d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_purpose_modified_df.createOrReplaceTempView(\"loans\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e1a1e412",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o242.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(loan_purpose#773, 200), ENSURE_REQUIREMENTS, [id=#298]\n+- *(1) HashAggregate(keys=[loan_purpose#773], functions=[partial_count(1)], output=[loan_purpose#773, count#810L])\n   +- *(1) Project [CASE WHEN loan_purpose#9 INSET (home_improvement,medical,credit_card,other,moving,renewable_energy,vacation,small_business,house,debt_consolidation,educational,wedding,major_purchase,car) THEN loan_purpose#9 ELSE other END AS loan_purpose#773]\n      +- *(1) Filter AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9)\n         +- FileScan csv [loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9] Batched: false, DataFilters: [AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_instal..., Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv015703/lendingclubproject/raw/loans_data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<loan_amount:float,funded_amount:float,loan_term_months:string,interest_rate:float,monthly_...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 42 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mtype_pprinters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_printers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                 deferred_pprinters=self.deferred_printers)\n\u001b[0;32m--> 702\u001b[0;31m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpretty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m             \u001b[0mprinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36mpretty\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    392\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                                 \u001b[0;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'__repr__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                             \u001b[0;32mreturn\u001b[0m \u001b[0m_repr_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_default_pprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/lib/pretty.py\u001b[0m in \u001b[0;36m_repr_pprint\u001b[0;34m(obj, p, cycle)\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;34m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    699\u001b[0m     \u001b[0;31m# Find newlines and replace them with p.break_()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 700\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    701\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplitlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m__repr__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    491\u001b[0m             return self._jdf.showString(\n\u001b[1;32m    492\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m                 self.sql_ctx._conf.replEagerEvalTruncate(), vertical)\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"DataFrame[%s]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\", \"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.showString.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(loan_purpose#773, 200), ENSURE_REQUIREMENTS, [id=#298]\n+- *(1) HashAggregate(keys=[loan_purpose#773], functions=[partial_count(1)], output=[loan_purpose#773, count#810L])\n   +- *(1) Project [CASE WHEN loan_purpose#9 INSET (home_improvement,medical,credit_card,other,moving,renewable_energy,vacation,small_business,house,debt_consolidation,educational,wedding,major_purchase,car) THEN loan_purpose#9 ELSE other END AS loan_purpose#773]\n      +- *(1) Filter AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9)\n         +- FileScan csv [loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9] Batched: false, DataFilters: [AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_instal..., Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv015703/lendingclubproject/raw/loans_data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<loan_amount:float,funded_amount:float,loan_term_months:string,interest_rate:float,monthly_...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:338)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 42 more\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o242.getRowsToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(loan_purpose#773, 200), ENSURE_REQUIREMENTS, [id=#328]\n+- *(1) HashAggregate(keys=[loan_purpose#773], functions=[partial_count(1)], output=[loan_purpose#773, count#810L])\n   +- *(1) Project [CASE WHEN loan_purpose#9 INSET (home_improvement,medical,credit_card,other,moving,renewable_energy,vacation,small_business,house,debt_consolidation,educational,wedding,major_purchase,car) THEN loan_purpose#9 ELSE other END AS loan_purpose#773]\n      +- *(1) Filter AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9)\n         +- FileScan csv [loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9] Batched: false, DataFilters: [AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_instal..., Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv015703/lendingclubproject/raw/loans_data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<loan_amount:float,funded_amount:float,loan_term_months:string,interest_rate:float,monthly_...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 42 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/beakerx/lib/python3.6/site-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    343\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36m_repr_html_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    505\u001b[0m             \u001b[0mmax_num_rows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql_ctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplEagerEvalMaxNumRows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m             sock_info = self._jdf.getRowsToPython(\n\u001b[0;32m--> 507\u001b[0;31m                 max_num_rows, self.sql_ctx._conf.replEagerEvalTruncate())\n\u001b[0m\u001b[1;32m    508\u001b[0m             \u001b[0mrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    509\u001b[0m             \u001b[0mhead\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrows\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1305\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1307\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.1.2-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o242.getRowsToPython.\n: org.apache.spark.sql.catalyst.errors.package$TreeNodeException: execute, tree:\nExchange hashpartitioning(loan_purpose#773, 200), ENSURE_REQUIREMENTS, [id=#328]\n+- *(1) HashAggregate(keys=[loan_purpose#773], functions=[partial_count(1)], output=[loan_purpose#773, count#810L])\n   +- *(1) Project [CASE WHEN loan_purpose#9 INSET (home_improvement,medical,credit_card,other,moving,renewable_energy,vacation,small_business,house,debt_consolidation,educational,wedding,major_purchase,car) THEN loan_purpose#9 ELSE other END AS loan_purpose#773]\n      +- *(1) Filter AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9)\n         +- FileScan csv [loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_installment#6,issue_date#7,loan_status#8,loan_purpose#9] Batched: false, DataFilters: [AtLeastNNulls(n, loan_amount#2,funded_amount#3,loan_term_months#4,interest_rate#5,monthly_instal..., Format: CSV, Location: InMemoryFileIndex[hdfs://m01.itversity.com:9000/user/itv015703/lendingclubproject/raw/loans_data_..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<loan_amount:float,funded_amount:float,loan_term_months:string,interest_rate:float,monthly_...\n\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:56)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.doExecute(ShuffleExchangeExec.scala:163)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.TakeOrderedAndProjectExec.executeCollect(limit.scala:187)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:3696)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:3687)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$5(SQLExecution.scala:103)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:163)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:90)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:775)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:64)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:3685)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:2722)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:2929)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:301)\n\tat org.apache.spark.sql.Dataset.getRowsToPython(Dataset.scala:3539)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\nsun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\nsun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\nsun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\njava.lang.reflect.Constructor.newInstance(Constructor.java:423)\npy4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\npy4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\npy4j.Gateway.invoke(Gateway.java:238)\npy4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\npy4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\npy4j.GatewayConnection.run(GatewayConnection.java:238)\njava.lang.Thread.run(Thread.java:748)\n\nThe currently active SparkContext was created at:\n\n(No active SparkContext.)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:118)\n\tat org.apache.spark.SparkContext.broadcast(SparkContext.scala:1506)\n\tat org.apache.spark.sql.execution.datasources.csv.CSVFileFormat.buildReader(CSVFileFormat.scala:102)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues(FileFormat.scala:130)\n\tat org.apache.spark.sql.execution.datasources.FileFormat.buildReaderWithPartitionValues$(FileFormat.scala:121)\n\tat org.apache.spark.sql.execution.datasources.TextBasedFileFormat.buildReaderWithPartitionValues(FileFormat.scala:170)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD$lzycompute(DataSourceScanExec.scala:407)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.inputRDD(DataSourceScanExec.scala:398)\n\tat org.apache.spark.sql.execution.FileSourceScanExec.doExecute(DataSourceScanExec.scala:485)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDD(WholeStageCodegenExec.scala:525)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs(WholeStageCodegenExec.scala:453)\n\tat org.apache.spark.sql.execution.InputRDDCodegen.inputRDDs$(WholeStageCodegenExec.scala:452)\n\tat org.apache.spark.sql.execution.InputAdapter.inputRDDs(WholeStageCodegenExec.scala:496)\n\tat org.apache.spark.sql.execution.FilterExec.inputRDDs(basicPhysicalOperators.scala:149)\n\tat org.apache.spark.sql.execution.ProjectExec.inputRDDs(basicPhysicalOperators.scala:50)\n\tat org.apache.spark.sql.execution.aggregate.HashAggregateExec.inputRDDs(HashAggregateExec.scala:141)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec.doExecute(WholeStageCodegenExec.scala:746)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$execute$1(SparkPlan.scala:180)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeQuery$1(SparkPlan.scala:218)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:215)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:176)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD$lzycompute(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.inputRDD(ShuffleExchangeExec.scala:118)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency$lzycompute(ShuffleExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.shuffleDependency(ShuffleExchangeExec.scala:149)\n\tat org.apache.spark.sql.execution.exchange.ShuffleExchangeExec.$anonfun$doExecute$1(ShuffleExchangeExec.scala:166)\n\tat org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:52)\n\t... 42 more\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select loan_purpose, count(*) as total from loans group by loan_purpose order by total desc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e1ad7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pyspark 3",
   "language": "python",
   "name": "pyspark3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
